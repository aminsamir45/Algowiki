{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# URL of the webpage containing the links to the datasets\n",
    "page_url = \"https://www.top500.org/path_to_page_containing_links/\"\n",
    "\n",
    "# Fetch the content of the webpage\n",
    "response = requests.get(page_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the links to the datasets (this depends on the structure of the webpage)\n",
    "# As an example, let's assume the links are within <a> tags with a specific class name\n",
    "links = [a['href'] for a in soup.find_all('a', class_='specific_class_name')]\n",
    "\n",
    "# Create an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through each link, download the data, and load it into a dataframe\n",
    "for link in links:\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "        data = BytesIO(response.content)\n",
    "        df = pd.read_csv(data)  # or pd.read_excel(data) if the data is in Excel format\n",
    "        dfs.append(df)\n",
    "\n",
    "# Combine all dataframes into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a CSV file\n",
    "combined_df.to_csv(\"combined_top500_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
